{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n 1.5005e-37  0.0000e+00 -7.6662e+02\n 4.5562e-41 -7.6663e+02  4.5562e-41\n 3.2602e-38  0.0000e+00  3.2603e-38\n 0.0000e+00  7.1108e+37  4.5562e-41\n-2.0679e+07  4.5562e-41  0.0000e+00\n[torch.FloatTensor of size 5x3]\n\n\n 0.4897  0.4298  0.3249\n 0.4832  0.4871  0.8124\n 0.8364  0.5842  0.3870\n 0.2969  0.0226  0.2087\n 0.5753  0.5417  0.4954\n[torch.FloatTensor of size 5x3]\n\n[[ 0.48969743  0.42983857  0.32488254]\n [ 0.48317397  0.48713782  0.81241864]\n [ 0.83640146  0.58417422  0.38703355]\n [ 0.29691932  0.02264219  0.20869948]\n [ 0.57528991  0.54174018  0.49538466]]\n\n 0.4897  0.4298  0.3249\n 0.4832  0.4871  0.8124\n 0.8364  0.5842  0.3870\n 0.2969  0.0226  0.2087\n 0.5753  0.5417  0.4954\n[torch.FloatTensor of size 5x3]\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n 0.4897  0.4298  0.3249\n 0.4832  0.4871  0.8124\n 0.8364  0.5842  0.3870\n 0.2969  0.0226  0.2087\n 0.5753  0.5417  0.4954\n[torch.cuda.FloatTensor of size 5x3 (GPU 0)]\n\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.Tensor(5, 3)\n",
    "print x\n",
    "\n",
    "x = torch.rand(5,3)\n",
    "print x\n",
    "\n",
    "n = x.numpy()\n",
    "print n\n",
    "\n",
    "t = torch.from_numpy(n)\n",
    "print t\n",
    "\n",
    "c = t.cuda()\n",
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n 1  1  1\n 1  1  1\n 1  1  1\n 1  1  1\n 1  1  1\n[torch.FloatTensor of size 5x3]\n\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "x1 = Variable(t, requires_grad=True)\n",
    "y = x1 * 5\n",
    "z = y.mean()\n",
    "grad = torch.ones(1)*3\n",
    "z.backward(grad)\n",
    "\n",
    "print x1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear (400 -> 120)\n  (fc2): Linear (120 -> 84)\n  (fc3): Linear (84 -> 10)\n)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n(0 ,.,.) = \n -0.1556  0.0840 -0.0577 -0.0516 -0.1619\n  0.1903  0.0136 -0.0816 -0.1010 -0.0973\n  0.1047  0.1158 -0.0016 -0.0184 -0.1950\n -0.1638 -0.1161 -0.0367  0.1942 -0.0288\n -0.1451  0.1651 -0.1666  0.0855  0.0575\n[torch.FloatTensor of size 1x5x5]\n\n[[[-0.15563506  0.08402465 -0.0576573  -0.05163998 -0.16187267]\n  [ 0.19033045  0.01363554 -0.08159894 -0.10103304 -0.09726713]\n  [ 0.10465418  0.11576439 -0.00160195 -0.01835244 -0.19499579]\n  [-0.16383953 -0.11605299 -0.0366952   0.19416147 -0.0288285 ]\n  [-0.14513239  0.16513522 -0.1665758   0.08552103  0.0574772 ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "params = list(net.parameters())\n",
    "len(params)\n",
    "conv1 = params[0]\n",
    "type(conv1)\n",
    "np_conv1 = conv1.data.numpy()\n",
    "print(conv1[0])\n",
    "print(np_conv1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n 28.7692\n[torch.FloatTensor of size 1]\n\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(1, 1, 32, 32))\n",
    "out = net(input)\n",
    "\n",
    "target = Variable(torch.arange(0, 10))  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(out, target)\n",
    "print loss\n",
    "#Sets gradients of all model parameters to zero.\n",
    "net.zero_grad() \n",
    "out.backward(torch.ones(1, 10), retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\nVariable containing:\n 0\n 0\n 0\n 0\n 0\n 0\n[torch.FloatTensor of size 6]\n\nconv1.bias.grad after backward\nVariable containing:\n1.00000e-02 *\n -0.9767\n  5.9167\n -4.7758\n  7.6609\n  3.9083\n  2.6372\n[torch.FloatTensor of size 6]\n\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}